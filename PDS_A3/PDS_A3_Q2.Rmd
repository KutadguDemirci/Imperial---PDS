---
title: \normalsize Assessment 3 \hfill MATH70094 Programming for Data Science \hfill Autumn 2025
    \rule{\linewidth}{0.5mm}
output:
  html_document: default
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kutadgu Gokalp Demirci - 06071571

# Question 2 (45 marks)

**The only non-base-R package you are allowed to load in this question
is testthat.**

Given a time series of length $n$ with data
$y_1,\dots,y_n\in\mathbb{R}$, we want to compare different ways for
forecasting future values $y_{k}$ of the time series at times $k>n$.
Depending on what statistical model we think has generated the data, a
different forecasting method is appropriate. Here, we consider two
statistical models:

(1) The time series was generated by $y_k = \epsilon_k$, where the
    $\epsilon_k$ are independent centred random variables. In this case,
    the time series has no trend and has expectation zero at all times.\
(2) The time series is an 'autoregressive process of order 1', where
    $y_{k+1} = \phi y_k + \epsilon_k$ for $k\in\mathbb{N}$ with a scalar
    $\phi$ and independent standard Gaussian random variables
    $\epsilon_k$ (more general autoregressive processes can be defined).
    The value of $\phi$ decides how the time series evolves over time.

Relative to the two models, we forecast future values of the time series
as follows:

(1) Using the rolling mean: Given a window length $h\in\mathbb{N}$,
    compute the 'rolling mean'
    $\bar{y}_h = h^{-1}(y_{n-h+1} + y_{n-h+2} + \dots + y_n)$. The
    predicted value of the time series at time $k>n$ is $\bar{y}_h$ for
    any $k$.
(2) The parameter $\phi$ can be estimated by
    $\hat{\phi} = \frac{\sum_{k=2}^n y_{k-1} y_k}{\sum_{k=2}^n (y_{k-1})^2}$.
    The predicted value of the time series at time $n+1$ is
    $\hat y_{n+1} = \hat\phi y_n$, the value at $n+2$ is
    $\hat y_{n+2} = \hat\phi \hat y_{n+1}$ etc.

Your task in this question is to write three different S3 classes in R
specified as follows:

1.  A class `TimeSeriesModel` with two methods `forecast` and
    `plot_forecast`. The two methods have no implementations, but both
    take as argument an integer `window_prediction_length` to indicate
    the number of future time steps that should be forecast. This class
    serves as a common parent class for the subclasses
    `RollingMeanModel` and `AR1Model` below. The constructor should not
    do anything as we will never instantiate objects from this class, so
    you can just use

```{r}
TimeSeriesModel <- function() {
  # TimeSeriesModel is abstract so use a subclass
}
```

2.  A subclass `RollingMeanModel` of `TimeSeriesModel` with

-   a constructor method that instantiates an object of
    `RollingMeanModel` given a time series (a `ts` object, check `?ts`)
    and an integer argument `window_train_length`; the latter is used to
    set the attribute `window_mean` that records the average value of
    the time series over the last `window_train_length` values,
-   a forecast method that overrides the forecast method from
    `TimeSeriesModel` and predicts the value of the time series for
    `window_prediction_length` future steps by `window_mean`,
-   a plot method `plot_forecast` that plots the time series model using
    the `plot.ts` function as well as the values of a 5-step prediction
    in the same graph.

3.  A subclass `AR1Model` of `TimeSeriesModel` with

-   a constructor method that instantiates an object of `AR1Model` given
    a time series; an object is instantiated by estimating the phi
    coefficient using the estimator $\hat\phi$ from above; the estimator
    is used to set an attribute of the same name,
-   a forecast method that overrides the forecast method from
    `TimeSeriesModel` and predicts the value of the time series for
    `window_prediction_length` future steps as follows: at time one in
    the future the prediction is $\hat y_{n+1}=\hat\phi*y_{n}$ with
    $y_{n}$ the last value of the training data, at time two by
    $\hat y_{n+2}=\hat\phi*\hat y_{n+1}$, etc,
-   a plot method `plot_forecast` that plots the time series model using
    the `plot.ts` function as well as the values of a 5-step prediction.

Below, you will be asked to write and use these classes in a test driven
development approach.

## Code clarity (3 mark)

There is a famous saying among software developers that code is read
more often than it is written. Marks will be awarded (or not awarded)
based on the clarity of the code and appropriate use of comments.

## Part A (10 marks)

Provide an interface (only class names, method names and empty method
bodies, S3-generics) for the three classes as specified above. Every
class and every method should have a short comment describing the
expected functionality. Do not provide implementations for the methods.

```{r}
# write the interfaces here
# Part A: Interfaces

# TimeSeriesModel is an abstract class
TimeSeriesModel <- function() {
  # TimeSeriesModel is abstract so use a subclass
  structure(list(), class = "TimeSeriesModel")
}

# Forecast method for TimeSeriesModel (empty)
forecast <- function(object, window_prediction_length, ...) {
  UseMethod("forecast")
}

forecast.TimeSeriesModel <- function(object, window_prediction_length, ...) {
  # Forecast future values of the time series for window_prediction_length steps
  stop("Abstract method. Use a subclass.")
}

# Plot forecast method for TimeSeriesModel (empty)
plot_forecast <- function(object, window_prediction_length, ...) {
  UseMethod("plot_forecast")
}

plot_forecast.TimeSeriesModel <- function(object, window_prediction_length, ...) {
  # Plot the time series and the forecast for window_prediction_length steps
  stop("Abstract method. Use a subclass.")
}

# RollingMeanModel subclass
RollingMeanModel <- function(time_series, window_train_length) {
  # Create a RollingMeanModel object
  # Args:
  #   time_series: a ts object
  #   window_train_length: integer for rolling mean window
  structure(list(
    time_series = time_series,
    window_train_length = window_train_length
  ), class = c("RollingMeanModel", "TimeSeriesModel"))
}

# Forecast method for RollingMeanModel (empty)
forecast.RollingMeanModel <- function(object, window_prediction_length, ...) {
  # Forecast using rolling mean - returns vector of predictions
  stop("Not implemented yet.")
}

# Plot forecast method for RollingMeanModel (empty)
plot_forecast.RollingMeanModel <- function(object, window_prediction_length, ...) {
  # Plot time series with 5-step forecast
  stop("Not implemented yet.")
}

# AR1Model subclass
AR1Model <- function(time_series) {
  # Create an AR1Model object
  # Args:
  #   time_series: a ts object
  structure(list(
    time_series = time_series
  ), class = c("AR1Model", "TimeSeriesModel"))
}

# Forecast method for AR1Model (empty)
forecast.AR1Model <- function(object, window_prediction_length, ...) {
  # Forecast using AR(1) model - returns vector of predictions
  stop("Not implemented yet.")
}

# Plot forecast method for AR1Model (empty)
plot_forecast.AR1Model <- function(object, window_prediction_length, ...) {
  # Plot time series with 5-step forecast
  stop("Not implemented yet.")
}

```

## Part B (10 marks)

Use the `testthat` library (you may have to install it first) to write
two test functions with `test_that` to test if your class codes work as
expected:

1.  For use during testing, create a time series object by defining a
    vector repeating the value 1 for 50 times and applying the `ts`
    function. Generate from this a `RollingMeanModel` object with
    `window_train_length` equal to 5, and an `AR1Model` object.
2.  A test function that checks if the models have been constructed
    correctly. There is some flexibility here. You could test for
    example if the internal variables have been set properly.
3.  A test function that checks if the model predictions have the
    correct length (as specified by `window_prediction_length`).

All tests should \emph{fail} at this point. Include `error=TRUE` in the
RMarkdown cell to prevent failing tests from stopping code execution.

```{r, error=TRUE}
library(testthat)

# Create test time series
test_ts <- ts(rep(1, 50))

# Create model objects (will have minimal structure but no proper implementation)
rolling_model <- RollingMeanModel(test_ts, 5)
ar1_model <- AR1Model(test_ts)

# Test 1: Check if models are constructed correctly
test_that("Models are constructed correctly", {
  # These should fail because implementations are empty
  expect_true("RollingMeanModel" %in% class(rolling_model))
  expect_true("AR1Model" %in% class(ar1_model))
  expect_equal(rolling_model$window_train_length, 5)
})

# Test 2: Check if model predictions have correct length
test_that("Model predictions have correct length", {
  # These should fail because forecast methods aren't implemented
  expect_length(forecast(rolling_model, 10), 10)
  expect_length(forecast(ar1_model, 10), 10)
})

# add your solution here
```

## Part C (10 marks)

Provide implementations for the three classes from part A. All tests
from part B should now pass without errors or failures.

```{r}
# add your solution here
# Part C: Implementations

# Update RollingMeanModel constructor
RollingMeanModel <- function(time_series, window_train_length) {
  n <- length(time_series)
  if (window_train_length > n) {
    stop("window_train_length is larger than time series length")
  }
  
  # Calculate rolling mean from last window_train_length observations
  recent_values <- tail(time_series, window_train_length)
  window_mean <- mean(recent_values)
  
  structure(list(
    time_series = time_series,
    window_train_length = window_train_length,
    window_mean = window_mean
  ), class = c("RollingMeanModel", "TimeSeriesModel"))
}

# Implement forecast for RollingMeanModel
forecast.RollingMeanModel <- function(object, window_prediction_length, ...) {
  rep(object$window_mean, window_prediction_length)
}

# Implement plot_forecast for RollingMeanModel
plot_forecast.RollingMeanModel <- function(object, window_prediction_length, ...) {
  # Use 5-step prediction as specified
  pred_length <- 5
  forecasts <- forecast(object, pred_length)
  
  # Combine original series with forecasts
  full_series <- ts(c(object$time_series, forecasts), 
                    start = start(object$time_series), 
                    frequency = frequency(object$time_series))
  
  # Plot with different colors for actual vs forecast
  plot.ts(full_series, main = "Rolling Mean Forecast")
  n_actual <- length(object$time_series)
  points(time(full_series)[(n_actual + 1):length(full_series)], 
         forecasts, col = "red", pch = 16)
  legend("topleft", legend = c("Actual", "Forecast"), 
         col = c("black", "red"), pch = c(1, 16))
}

# Update AR1Model constructor
AR1Model <- function(time_series) {
  n <- length(time_series)
  if (n < 2) {
    stop("Time series must have at least 2 observations for AR(1) estimation")
  }
  
  # Estimate phi using the given formula
  y_lag <- time_series[1:(n-1)]
  y_current <- time_series[2:n]
  phi_hat <- sum(y_lag * y_current) / sum(y_lag^2)
  
  structure(list(
    time_series = time_series,
    phi_hat = phi_hat,
    last_value = tail(time_series, 1)
  ), class = c("AR1Model", "TimeSeriesModel"))
}

# Implement forecast for AR1Model
forecast.AR1Model <- function(object, window_prediction_length, ...) {
  forecasts <- numeric(window_prediction_length)
  current <- object$last_value
  
  for (i in 1:window_prediction_length) {
    current <- object$phi_hat * current
    forecasts[i] <- current
  }
  
  forecasts
}

# Implement plot_forecast for AR1Model
plot_forecast.AR1Model <- function(object, window_prediction_length, ...) {
  # Use 5-step prediction as specified
  pred_length <- 5
  forecasts <- forecast(object, pred_length)
  
  # Combine original series with forecasts
  full_series <- ts(c(object$time_series, forecasts), 
                    start = start(object$time_series), 
                    frequency = frequency(object$time_series))
  
  # Plot with different colors for actual vs forecast
  plot.ts(full_series, main = "AR(1) Forecast")
  n_actual <- length(object$time_series)
  points(time(full_series)[(n_actual + 1):length(full_series)], 
         forecasts, col = "red", pch = 16)
  legend("topleft", legend = c("Actual", "Forecast"), 
         col = c("black", "red"), pch = c(1, 16))
}

```

## Part D (12 marks)

### Part D(i) (8 marks)

Add a function `compare_models` (not part of any class) that takes as
arguments a time series `train`, a time series `test` and a
`window_train_length`. Within the function, instantiate from the `train`
time series a `RollingMeanModel` object and an `AR1Model` object. The
`compare_models` function should then return TRUE if the mean square
error between `test` and the forecast of the same length by the
`RollingMeanModel` object is smaller than the mean square error between
`test` and the forecast of the same length by the `AR1Model` object.

Write also a test function (again using `test_that`) that checks if the
`compare_models` function works as expected (and ideally write the test
method first). The test should pass.

```{r}
# add your solution here
# Part D(i): compare_models function

compare_models <- function(train, test, window_train_length) {
  # Create models
  rolling_model <- RollingMeanModel(train, window_train_length)
  ar1_model <- AR1Model(train)
  
  # Generate forecasts of same length as test data
  n_test <- length(test)
  rolling_forecast <- forecast(rolling_model, n_test)
  ar1_forecast <- forecast(ar1_model, n_test)
  
  # Calculate mean squared errors
  mse_rolling <- mean((test - rolling_forecast)^2)
  mse_ar1 <- mean((test - ar1_forecast)^2)
  
  # Return TRUE if RollingMeanModel has smaller MSE
  return(mse_rolling < mse_ar1)
}

# Test for compare_models function
test_that("compare_models works correctly", {
  # Create simple test case
  train_data <- ts(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1))
  test_data <- ts(c(1, 1, 1, 1, 1))
  
  # Both models should predict 1s, so MSE should be 0 for both
  # Therefore compare_models should return FALSE (0 < 0 is FALSE)
  result <- compare_models(train_data, test_data, 5)
  expect_false(result)
  
  # Test with different data where rolling mean should be better
  train_data2 <- ts(c(2, 2, 2, 2, 2, 2, 2, 2, 2, 2))
  test_data2 <- ts(c(2, 2, 2, 2, 2))
  result2 <- compare_models(train_data2, test_data2, 5)
  expect_false(result2)  # Still equal MSEs
})

```

### Part D(ii) (4 marks)

Read in the data from the file `time_series_data.csv`. Split this data
into two data sets and create two time series objects `train` and `test`
using the `ts` function, where `train` consists of the first 150 data
points and `test` of the remaining data. Using the `compare_models`
functions from Part D(i), which model (out of `RollingMeanModel` and
`AR1Model`) performs better in terms of forecasting for this data set?

```{r}
# add your solution here
# Part D(ii): Apply to real data

# Read the data from file
time_series_data <- read.csv("time_series_data.csv")

# Assuming the data has one column, convert to vector
if (ncol(time_series_data) == 1) {
  data_vector <- time_series_data[, 1]
} else {
  # If multiple columns, use the first column
  data_vector <- time_series_data[, 1]
}

# Split into train (first 150) and test (remaining)
train <- ts(data_vector[1:150])
test <- ts(data_vector[151:length(data_vector)])

# Compare models
rolling_better <- compare_models(train, test, window_train_length = 5)

# Print result
if (rolling_better) {
  print("RollingMeanModel performs better for this dataset")
} else {
  print("AR1Model performs better for this dataset")
}

# Optional: Show some model details for insight
rolling_model <- RollingMeanModel(train, 5)
ar1_model <- AR1Model(train)

cat("Rolling mean:", rolling_model$window_mean, "\n")
cat("AR(1) phi estimate:", ar1_model$phi_hat, "\n")
```
